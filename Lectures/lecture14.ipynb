{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lecture14.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "zZMAcS81kTAI"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wdconinc/practical-computing-for-scientists/blob/master/Lectures/lecture14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "gq58FBlp6tgl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lecture #14"
      ]
    },
    {
      "metadata": {
        "id": "t9bpzU8h6tgn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Standard Preamble"
      ]
    },
    {
      "metadata": {
        "id": "v5a6Xkvl6tgp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy.matlib as ml"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B203qnmN6tgs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##In our last episode"
      ]
    },
    {
      "metadata": {
        "id": "7ZSSKjx66tgt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Newton's method in 2D: an example\n",
        "* The partial derivative of a function of more than one variable\n",
        "* The gradient vector\n",
        "* The Hessian matrix"
      ]
    },
    {
      "metadata": {
        "id": "XGtVH0IAGnsf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Newton method for scalar functions of one variable"
      ]
    },
    {
      "metadata": {
        "id": "WGgJfp7pGi8-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you know, from calculus, we can expand any well behaved function $f(x)$ in a series around a point $\\bar{x}$:\n",
        "\n",
        "$$f(x) = f(\\bar{x}) + \\frac{\\partial f}{\\partial x}\\big|_\\bar{x}(x-\\bar{x}) + \\frac{\\partial^2 f}{\\partial x^2}\\big|_\\bar{x} \\frac{1}{2}(x-\\bar{x})^2 $$\n",
        "\n",
        "or with $\\Delta x = x - \\bar{x}$\n",
        "\n",
        "\n",
        "$$f(x) = f(\\bar{x} + \\Delta x) = f(\\bar{x}) + \\frac{\\partial f}{\\partial x}\\big|_\\bar{x} \\Delta x + \\frac{\\partial^2 f}{\\partial x^2}\\big|_\\bar{x} \\frac{1}{2} \\Delta x^2 $$"
      ]
    },
    {
      "metadata": {
        "id": "UcEAdMbDexlL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will successively improve our estimate of the minimum by choosing different points $\\bar{x}_n$ that are closer to the minimum. We want $\\bar{x}_n$ to be a stationary point, so that\n",
        "$$ \\frac{df(\\bar{x} + \\Delta x)}{d\\Delta x} = 0$$\n",
        "\n",
        "This happens when \n",
        "$$ \\frac{\\partial f}{\\partial x}\\big|_{\\bar{x}_n} + \\frac{\\partial^2 f}{\\partial x^2}\\big|_{\\bar{x}_n} \\Delta x = 0 $$\n",
        "\n",
        "So we update successively with this $\\Delta x$:\n",
        "$$ \\bar{x}_{n+1} = \\bar{x}_n + \\Delta x =\\bar{x}_n - \\frac{\\partial f}{\\partial x}\\big|_{\\bar{x}_n} / \\frac{\\partial^2 f}{\\partial x^2}\\big|_{\\bar{x}_n} $$\n"
      ]
    },
    {
      "metadata": {
        "id": "QEkYIbP_Gi9B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Newton method for scalar functions of more than one variable"
      ]
    },
    {
      "metadata": {
        "id": "1Ko_FW_HGQbX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In analogy with the one-dimensional function increment $\\Delta x = -f'(x) / f''(x)$ we now want\n",
        "\n",
        "$$ \\Delta\\vec{x} = - [H f(\\vec{x})]^{-1} \\nabla f(\\vec{x}) $$"
      ]
    },
    {
      "metadata": {
        "id": "yekJw0Y36tgv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###The partial derivative"
      ]
    },
    {
      "metadata": {
        "id": "LnO9GPRATzW6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$$ \\frac{\\partial f}{\\partial x_i} $$"
      ]
    },
    {
      "metadata": {
        "id": "lhdcnUcm6tgw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def partial(f, i, h = 1e-6):\n",
        "    ''' \n",
        "        Returns a function object to compute the partial derivative of f with respect to x[i].\n",
        "        \n",
        "        f(x) is assumed to be a scalar function of a vector or scalar argument x.\n",
        "    '''\n",
        "    def df(x, f = f, i = i, h = h):\n",
        "        x = np.array(x, dtype = np.float64) # make a copy and assure the use of 64-bit floats\n",
        "        x[i] += h\n",
        "        f_plus = f(x)\n",
        "        x[i] -= 2*h\n",
        "        f_minus = f(x)\n",
        "        return (f_plus - f_minus) / (2.0 * h)\n",
        "    # note, partial() returns a function object, not the result of the function\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YBmXaBsU_31g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The gradient"
      ]
    },
    {
      "metadata": {
        "id": "BBc7hzqyGcG7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$$ \\mathbf{\\nabla} f = \\begin{bmatrix}\n",
        "\\dfrac{\\partial f}{\\partial x_{1}} \\\\[2.2ex]\n",
        "\\dfrac{\\partial f}{\\partial x_{2}} \\\\[2.2ex]\n",
        "\\vdots \\\\[2.2ex]\n",
        "\\dfrac{\\partial f}{\\partial x_{n}}\n",
        "\\end{bmatrix}$$"
      ]
    },
    {
      "metadata": {
        "id": "k_8zTMaz_31l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gradient(f, x, h = 1e-6):\n",
        "    ''' return the gradient of f(x) as a column vector with length = len(x) '''\n",
        "    v = [ partial(f, i, h = h)(x)\n",
        "         for i in range(len(x)) ]\n",
        "\n",
        "    return ml.matrix(v, dtype = np.float64).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C7S0FOvn_31q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Hessian matrix"
      ]
    },
    {
      "metadata": {
        "id": "mIuC19CyY80w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$$\\mathbf{H} f = \\begin{bmatrix}\n",
        "\\dfrac{\\partial^{2}f}{\\partial x_{1}^{2}} & \\dfrac{\\partial^{2}f}{\\partial x_{1}\\,\\partial x_{2}} & \\cdots & \\dfrac {\\partial^{2}f}{\\partial x_{1}\\,\\partial x_{n}} \\\\[2.2ex]\n",
        "\\dfrac{\\partial^{2}f}{\\partial x_{2}\\,\\partial x_{1}} & \\dfrac {\\partial^{2}f}{\\partial x_{2}^{2}} & \\cdots & \\dfrac {\\partial^{2}f}{\\partial x_{2}\\,\\partial x_{n}} \\\\[2.2ex]\n",
        "\\vdots & \\vdots &\\ddots &\\vdots \\\\[2.2ex]\n",
        "\\dfrac{\\partial^{2}f}{\\partial x_{n}\\,\\partial x_{1}} & \\dfrac {\\partial^{2}f}{\\partial x_{n}\\,\\partial x_{2}} & \\cdots & \\dfrac {\\partial^{2}f}{\\partial x_{n}^{2}}\n",
        "\\end{bmatrix}$$"
      ]
    },
    {
      "metadata": {
        "id": "S97ceSpw_31q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def hessian(f, x, h = 1e-6):\n",
        "    ''' returns the two dimensional matrix of second partial derivatives of f(x). a.k.a. the Hessian matrix '''\n",
        "    v = [ \n",
        "          [ \n",
        "              partial(partial(f,column),row)(x)\n",
        "           for column in range(len(x)) ]\n",
        "        for row in range(len(x)) ]\n",
        "    \n",
        "    return ml.matrix(v, dtype = np.float64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MI_YQ_OV_32L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####A test function: $f(\\vec{x})=(x-1)^2 + (y-2)^2$ "
      ]
    },
    {
      "metadata": {
        "id": "1gtE2z_PZxZ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using obscure `numpy.mgrid` tricks instead of `numpy.meshgrid`... See https://docs.scipy.org/doc/numpy/reference/generated/numpy.mgrid.html"
      ]
    },
    {
      "metadata": {
        "id": "R3jUp4RB_32M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f = lambda x : (x[0] - 1)**2 + (x[1] - 2)**2\n",
        "x0, x1 = np.mgrid[-2.5:3:101j, 0:4:101j]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xv4dTAqEKm5t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(x0, x1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cznpFdnzK_p-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.contourf(x0, x1, f([x0,x1]), 20)\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ecreO3eE_32T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####The `multi_newton` function"
      ]
    },
    {
      "metadata": {
        "id": "SexCzUvcJqYz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Remember what we are trying to obtain! In analogy with the one-dimensional function increment $\\Delta x = -f'(x) / f''(x)$ we now want\n",
        "\n",
        "$$ \\Delta\\vec{x} = - [H f(\\vec{x})]^{-1} \\nabla f(\\vec{x}) $$"
      ]
    },
    {
      "metadata": {
        "id": "tJfAHwvM_32W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def multi_newton(f, xguess, h = 1e-6, accuracy = 1e-6, nmax = 100, want_points = False, debug = False, gamma = 1.0):\n",
        "    xbar = ml.matrix(xguess, dtype = np.float64).T # to get a column vector\n",
        "    xpoints = [xbar.A1[0]]\n",
        "    ypoints = [xbar.A1[1]]\n",
        "    for i in range(nmax):\n",
        "        H = hessian(f, xbar.A1)\n",
        "        grad = gradient(f, xbar.A1) # to get a column vector\n",
        "        if debug:\n",
        "            print(\"=========\")\n",
        "            print(\"iteration: \", i)\n",
        "            print(\"xbar =\", xbar)\n",
        "            print(\"grad =\", grad)\n",
        "            print(\"H =\", H)\n",
        "            print(\"H.I =\", H.I)\n",
        "            print(\"H.I.dot(grad) =\", H.I.dot(grad))\n",
        "        x = xbar - gamma * H.I.dot(grad)\n",
        "        if debug: print(\"x =\", x)\n",
        "        xpoints.append(x.A1[0])\n",
        "        ypoints.append(x.A1[1])\n",
        "        if np.sum((x.A1 - xbar.A1)**2) < accuracy**2:\n",
        "            if not want_points:\n",
        "                return x.A1\n",
        "            else:\n",
        "                return x.A1, xpoints, ypoints\n",
        "        else:\n",
        "            xbar = x\n",
        "    raise ArithmeticError(\"Failed to converge\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0D0UDYazLutN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "multi_newton(f, [1,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xie4_82eLzFR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "multi_newton(f, [1,1], debug = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yF4YqGIwK3Pb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "minx, x0points, x1points = multi_newton(f, [1,1], want_points = True, gamma = 1.0)\n",
        "\n",
        "plt.contourf(x0, x1, f([x0,x1]), 20)\n",
        "plt.colorbar()\n",
        "\n",
        "plt.plot(x0points, x1points, '-or')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TIrFa5cV_32b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### A somewhat more difficult test function: $ f(\\vec{x}) = 1 - x_0 e^{-x_0} + (x_1 - 2)^2 $\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yJggLZpe_32c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fharder = lambda x : 1 - x[0] * np.exp(-x[0]) + (x[1] - 2)**2\n",
        "x0, x1 = np.mgrid[-2.5:4:101j, 0:4:101j]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cOmvqMSeVVAa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.contourf(x0, x1, fharder([x0,x1]), 20)\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EFWLx7TEVKpQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.contour(x0, x1, fharder([x0,x1]), 100)\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YsMNEjiB_32f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Testing `multi_newton` on the new test function"
      ]
    },
    {
      "metadata": {
        "id": "rWLq2cdq_32f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "minx, x0points, x1points = multi_newton(fharder, [1.8, 2.8], want_points = True)\n",
        "\n",
        "plt.contour(x0, x1, fharder([x0,x1]), 100)\n",
        "plt.colorbar()\n",
        "\n",
        "print(x0points, x1points)\n",
        "plt.plot(x0points, x1points, \"-or\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e2iTSCu8MNxB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "minx = multi_newton(fharder, [1.8, 2.8], debug = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MA_N0gjTWA1c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "minx, x0points, x1points = multi_newton(fharder, [1.8, 2.8], want_points = True, gamma = 0.5)\n",
        "\n",
        "plt.contour(x0, x1, fharder([x0,x1]), 100)\n",
        "plt.colorbar()\n",
        "\n",
        "print(x0points, x1points)\n",
        "plt.plot(x0points, x1points, \"-or\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p_SMnM8u6thu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Other multi-dimensional minimization algorithms you will encounter"
      ]
    },
    {
      "metadata": {
        "id": "X9eG9JNIfr9P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Gradient-Descent Algorithm"
      ]
    },
    {
      "metadata": {
        "id": "Bb-zm9Gjfzy9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have seen how the Hessian can both improve and degrade the performance of the multi-dimensional Newton's algorithm. In the gradient descent algorithm, we just use the gradient and (essentially) set the Hessian to an identity matrix.\n",
        "\n",
        "$$ \\Delta\\vec{x} = - \\nabla f(\\vec{x}) $$"
      ]
    },
    {
      "metadata": {
        "id": "75PknyOygTP5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def multi_gradient(f, xguess, h = 1e-6, accuracy = 1e-6, nmax = 100, want_points = False, debug = False, gamma = 1.0):\n",
        "    xbar = ml.matrix(xguess, dtype = np.float64).T # to get a column vector\n",
        "    xpoints = [xbar.A1[0]]\n",
        "    ypoints = [xbar.A1[1]]\n",
        "    for i in range(nmax):\n",
        "        grad = gradient(f, xbar.A1) # to get a column vector\n",
        "        if debug:\n",
        "            print(\"=========\")\n",
        "            print(\"iteration: \", i)\n",
        "            print(\"xbar =\", xbar)\n",
        "            print(\"grad =\", grad)\n",
        "        x = xbar - gamma * grad\n",
        "        if debug: print(\"x =\", x)\n",
        "        xpoints.append(x.A1[0])\n",
        "        ypoints.append(x.A1[1])\n",
        "        if np.sum((x.A1 - xbar.A1)**2) < accuracy**2:\n",
        "            if not want_points:\n",
        "                return x.A1\n",
        "            else:\n",
        "                return x.A1, xpoints, ypoints\n",
        "        else:\n",
        "            xbar = x\n",
        "    raise ArithmeticError(\"Failed to converge\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2aQJATtIgkBX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####A test function: $f(\\vec{x})=(x-1)^2 + (y-2)^2$ "
      ]
    },
    {
      "metadata": {
        "id": "lQ4yp5iIgkBZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f = lambda x : (x[0] - 1)**2 + (x[1] - 2)**2\n",
        "x0, x1 = np.mgrid[-2.5:3:101j, 0:4:101j]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1eckjy8bgkBd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.contourf(x0, x1, f([x0,x1]), 20)\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sCRH60TugTP-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "multi_gradient(f, [1,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7_dyu8HAgTQD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "multi_gradient(f, [1,1], gamma = 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9QcomKmXhuwu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "multi_gradient(f, [1,1], debug = True, gamma = 0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sX56U3v7h2NA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "multi_gradient(f, [1,1], debug = True, gamma = 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r0CJrVu0gTQI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "minx, x0points, x1points = multi_gradient(f, [1,1], want_points = True, gamma = 0.25)\n",
        "\n",
        "plt.contourf(x0, x1, f([x0,x1]), 20)\n",
        "plt.colorbar()\n",
        "\n",
        "plt.plot(x0points, x1points, '-or')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jYLJJpFYgTQT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### A somewhat more difficult test function: $ f(\\vec{x}) = 1 - x_0 e^{-x_0} + (x_1 - 2)^2 $\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "JR6dj3thgTQV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fharder = lambda x : 1 - x[0] * np.exp(-x[0]) + (x[1] - 2)**2\n",
        "x0, x1 = np.mgrid[-2.5:4:101j, 0:4:101j]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O-nMM-jMgTQX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.contourf(x0, x1, fharder([x0,x1]), 20)\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BydCT_VigTQf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.contour(x0, x1, fharder([x0,x1]), 100)\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f9fjf1HIgTQl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Testing `multi_gradient` on the new test function"
      ]
    },
    {
      "metadata": {
        "id": "BrCbVGxTgTQl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "minx, x0points, x1points = multi_gradient(fharder, [1.8, 2.8], want_points = True, gamma = 0.35)\n",
        "\n",
        "plt.contour(x0, x1, fharder([x0,x1]), 100)\n",
        "plt.colorbar()\n",
        "\n",
        "print(x0points, x1points)\n",
        "plt.plot(x0points, x1points, \"-or\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FJF46ChNiOMW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "minx = multi_gradient(fharder, [1.8, 2.8], gamma = 0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c1nkGbqLgTQo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "minx = multi_gradient(fharder, [1.8, 2.8], debug = True, gamma = 0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a_XiT_ylgTQu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "minx, x0points, x1points = multi_gradient(fharder, [1.8, 2.8], want_points = True, gamma = 0.35)\n",
        "\n",
        "plt.contour(x0, x1, fharder([x0,x1]), 100)\n",
        "plt.colorbar()\n",
        "\n",
        "print(x0points, x1points)\n",
        "plt.plot(x0points, x1points, \"-or\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "On36BzIwjPMo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### An even more difficult test function: $ f(\\vec{x}) = (1 - x_0)^2 + 100 (x_1 - x_0^2)^2 $\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "h-Kcmkw5jPMq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fharder = lambda x : (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n",
        "x0, x1 = np.mgrid[-1.0:1.5:101j, -0.5:1.5:101j]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n8xGasEhkr0n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is the Rosenbrock function. It has a minimum at $(1,1)$ where $f(\\vec{x}) = 0$. Everywhere else, $f(\\vec{x}) > 0$."
      ]
    },
    {
      "metadata": {
        "id": "99S9ulSxk1ix",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(fharder([1, 1]))\n",
        "print(fharder([1.01, 1.01]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o03Qtt5PjPMs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.contourf(x0, x1, fharder([x0,x1]), 20)\n",
        "plt.plot(1, 1, 'ob')\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Dj332ZjjPMv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.contour(x0, x1, fharder([x0,x1]), 100)\n",
        "plt.plot(1, 1, 'ob')\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FqWz7HxYj5Rf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Testing `multi_gradient` on the new test function"
      ]
    },
    {
      "metadata": {
        "id": "CZ_nyt9Yj5Ri",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "minx, x0points, x1points = multi_gradient(fharder, [0.0, 0.0], want_points = True, debug = True, gamma = 0.05)\n",
        "\n",
        "plt.contour(x0, x1, fharder([x0,x1]), 100)\n",
        "plt.plot(0, 0, 'or')\n",
        "plt.plot(1, 1, 'ob')\n",
        "plt.colorbar()\n",
        "\n",
        "print(x0points, x1points)\n",
        "plt.plot(x0points, x1points, \"-or\")\n",
        "plt.xlim(-1, 1)\n",
        "plt.ylim(-0.5, 1.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zZMAcS81kTAI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Gradient descent with automatic determination of $\\gamma_n$ in each step"
      ]
    },
    {
      "metadata": {
        "id": "riqlE-n9I0wK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Minimization methods for multidimensional curve fitting: a specific subset of a general problem"
      ]
    },
    {
      "metadata": {
        "id": "v4wANCDMI7b6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We want to determine the minimum of the function $\\chi^2(\\{\\vec{x}_i\\}, \\{y_i\\}, \\{\\delta y_i\\}, \\vec{\\theta})$ for a set of data points $\\{y_i\\}$ at a set of independent variables $\\{\\vec{x}_i\\}$. We want this minimum by changing the parameter vector $\\vec{\\theta}$.\n",
        "\n",
        "$$ \\chi^2(\\{\\vec{x}_i\\}, \\{y_i\\}, \\vec{\\theta}) = \\sum_i \\frac{\\big(y_i - f(\\vec{x}_i, \\theta)\\big)^2}{\\delta y_i^2} $$\n",
        "\n",
        "We want to find $\\vec{\\theta}$ such that $\\chi^2$ is minimized. Think of $\\chi^2$ as a function of $\\vec{\\theta}$ only, with given $\\{\\vec{x}_i\\}, \\{y_i\\}$."
      ]
    },
    {
      "metadata": {
        "id": "llzUJ7ZzLdrc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's create a set of independent variables, $\\vec{x}_i$\n",
        "."
      ]
    },
    {
      "metadata": {
        "id": "R0lndiHiLXQm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N = 100 # number of observations\n",
        "D = 1 # dimensionality of independent variables\n",
        "\n",
        "x = np.random.rand(N, D)\n",
        "print(x[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0JXkAj2uMu2j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now create a set of observations, $y_i$, based on an exact linear dependence.\n",
        "\n",
        "$$ y_i = f(\\vec{x}_i,\\vec{\\theta}) = \\sum_{k=0}^{D-1} \\theta_k (\\vec{x}_i)_k + \\theta_D = \\vec\\theta \\cdot \\vec{x}_i + \\theta_D$$"
      ]
    },
    {
      "metadata": {
        "id": "9k5WhotXYuUJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "theta_exact = [2.0] * D + [3.0]\n",
        "print(theta_exact)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O-Vff6mYMaV8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f = lambda x, theta: theta[D] + np.matmul(x, theta[:D])\n",
        "\n",
        "y_exact = f(x, theta_exact)\n",
        "\n",
        "delta_y = 0.25 # measurement uncertainty on y\n",
        "y_observed = np.random.normal(y_exact, delta_y)\n",
        "\n",
        "print(y_exact[:10])\n",
        "print(y_observed[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IO46UOFePn1_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.errorbar(x[:,0], y_observed, delta_y, None, 'ok', ms = 6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9k3SeyXrbYmv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def chi2(theta):\n",
        "  print(np.shape(y_observed), np.shape(x), np.shape(theta), np.shape(f(x,theta)))\n",
        "  res = y_observed - f(x, theta)\n",
        "  return np.sum(res**2 / delta_y**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aq_TTwdpLbM9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_observed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gxbb06zabnYl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "theta = [[1,2], [2,3]]\n",
        "print(theta[:D], theta[D])\n",
        "print(chi2(theta))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tXhcbVJPb9E1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "theta0, theta1 = np.mgrid[-4.0:4.0:201j, -4.0:4.0:201j]\n",
        "plt.contour(theta0, theta1, chi2([theta0, theta1]), 100)\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-wV5KCvr6thw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Levenberg-Marquardt Algorithm"
      ]
    },
    {
      "metadata": {
        "id": "T2laF1vd6thy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is used in the `scipy.optimize.leastsq` routine.\n",
        "\n",
        "We want to minimize a sum of squares:\n",
        "\n",
        "$$ \\S \\sum_i^N (y_i -f(x_i,\\theta))^2 $$"
      ]
    },
    {
      "metadata": {
        "id": "3Ut1evyr6thy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}