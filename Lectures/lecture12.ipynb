{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lecture12.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "_nqjJXHFGnsP",
        "BYa52EzFGnsQ",
        "BsQoSY2MGnsT",
        "_WyFMA7QbRhJ",
        "pS_cxH-PGnsV",
        "fQT8s2bKOR1z",
        "D2WncvmRGnsZ",
        "7z9st6ygGnsd",
        "XGtVH0IAGnsf",
        "HolX9gDdP0xx",
        "F6EBwq2eGi9U",
        "xNnKHFixP0xy",
        "phqQX1w3P0x4",
        "h-QWR2IGGnsi",
        "82zft3JSGi9B",
        "L7kcep_WGi9E",
        "YBmXaBsU_31g",
        "C7S0FOvn_31q",
        "MI_YQ_OV_32L",
        "ecreO3eE_32T",
        "TIrFa5cV_32b",
        "YsMNEjiB_32f"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wdconinc/practical-computing-for-scientists/blob/master/Lectures/lecture12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_nqjJXHFGnsP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Lecture #12"
      ]
    },
    {
      "metadata": {
        "id": "BYa52EzFGnsQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Standard Preamble"
      ]
    },
    {
      "metadata": {
        "id": "6F6BEQGtGnsQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BsQoSY2MGnsT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##In our last episode"
      ]
    },
    {
      "metadata": {
        "id": "roKmee6oGnsT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Finding roots of functions of single variable\n",
        "* Optimization\n",
        "  * Connection between root finding and 1D optimization.\n",
        "  * Finding minima and maxima of functions.\n"
      ]
    },
    {
      "metadata": {
        "id": "_WyFMA7QbRhJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Warm-up exercise"
      ]
    },
    {
      "metadata": {
        "id": "5pYOD8smbZqv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "message = \"Yrg'f unir gur cebwrpg qhr ba Zbaqnl vafgrnq.\"\n",
        "\n",
        "abc = \"abcdefghijklmnopqrstuvwxyzYZ\"\n",
        "abc13 = abc[13:26] + abc[:13] + \"LM\"\n",
        "\n",
        "rot13 = str.maketrans(abc, abc13)\n",
        "str.translate(message, rot13)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pS_cxH-PGnsV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Optimization"
      ]
    },
    {
      "metadata": {
        "id": "rI5P5ACiGnsV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\"Optimization\" refers to the process in which we minimimze or maximize a quantity. Finding the root of the derivative.\n",
        "\n",
        "Exercise:\n",
        "\n",
        "Find the minima and maxima of our test function\n",
        "\n",
        "$$ f(x) = \\cos 8x + x^2 -x $$"
      ]
    },
    {
      "metadata": {
        "id": "BlMYzgPfGnsV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "g = lambda x : np.cos(8*x) + x**2 - x\n",
        "dg = lambda x : -8*np.sin(8*x) + 2*x - 1\n",
        "x = np.linspace(-1,1.5,100)\n",
        "plt.plot(x,g(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fQT8s2bKOR1z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Local or global minima within a range"
      ]
    },
    {
      "metadata": {
        "id": "Ib3BFGzXOUi1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/68/Extrema_example_original.svg\">"
      ]
    },
    {
      "metadata": {
        "id": "D2WncvmRGnsZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Bracketing minima"
      ]
    },
    {
      "metadata": {
        "id": "pnk7zXkHGnsa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def bracket_minima(f, a, b, ndivisions = 100):\n",
        "    ''' \n",
        "    return a list of tuples (xlow, xmid, xhigh) that bracket each minimum\n",
        "    '''\n",
        "    result = []\n",
        "    dx = (b - a) / ndivisions\n",
        "    xfirst = a\n",
        "    for i in range(ndivisions*2):\n",
        "        xmid = xfirst + dx / 2.0\n",
        "        xlast = xmid + dx / 2.0\n",
        "        if f(xfirst) > f(xmid) and f(xlast) > f(xmid):\n",
        "            # we have a minimum\n",
        "            result.append((xfirst, xmid, xlast))\n",
        "        xfirst = xmid\n",
        "    return result\n",
        "\n",
        "bracket_minima(g, -1.0, 1.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7z9st6ygGnsd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Improving knowledge of the minima: golden ratio search"
      ]
    },
    {
      "metadata": {
        "id": "qDos_Wq2ClZn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/52/GoldenSectionSearch.png\" width=\"400\">"
      ]
    },
    {
      "metadata": {
        "id": "_Fsp5vRIGnsd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def golden_search(f, x1, x3, accuracy = 1e-6, nmax = 100):\n",
        "    tau = (math.sqrt(5) - 1) / 2.0\n",
        "    x2 = x1 + (x3 - x1) * (1 - tau)\n",
        "    x4 = x1 + (x3 - x1) * tau\n",
        "    f1, f2, f4, f3 = f(x1), f(x2), f(x4), f(x3)\n",
        "    for i in range(nmax):\n",
        "        print(\"iteration %i\" % i)\n",
        "        print(\"x1=%f, x2=%f, x4=%f, x3=%f\" % (x1,x2,x4,x3))\n",
        "        print(\"f1=%f, f2=%f, f4=%f, f3=%f\" % (f1,f2,f4,f3))\n",
        "\n",
        "        if f2 < f4: # f4a\n",
        "            x3, f3, x4, f4 = x4, f4, x2, f2\n",
        "            x2 = x1 + (x3 - x1) * (1 - tau)\n",
        "            f2 = f(x2)\n",
        "        else:       # f4b\n",
        "            x1, f1, x2, f2 = x2, f2, x4, f4\n",
        "            x4 = x1 + (x3 - x1) * tau\n",
        "            f4 = f(x4)\n",
        "\n",
        "        if math.fabs(x4 - x1) < accuracy: return x4\n",
        "\n",
        "f = lambda x : (x - 0.25)**2\n",
        "\n",
        "golden_search(f, 0.0, 1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XGtVH0IAGnsf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Newton's method for optimization"
      ]
    },
    {
      "metadata": {
        "id": "WGgJfp7pGi8-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you know, from calculus, we can expand any well behaved function $f(x)$ in a series around a point $\\bar{x}$:\n",
        "\n",
        "$$f(x) = f(\\bar{x}) + \\frac{\\partial f}{\\partial x}\\big|_\\bar{x}(x-\\bar{x}) + \\frac{\\partial^2 f}{\\partial x^2}\\big|_\\bar{x} \\frac{1}{2}(x-\\bar{x})^2 $$\n",
        "\n",
        "or with $\\Delta x = x - \\bar{x}$\n",
        "\n",
        "\n",
        "$$f(x) = f(\\bar{x} + \\Delta x) = f(\\bar{x}) + \\frac{\\partial f}{\\partial x}\\big|_\\bar{x} \\Delta x + \\frac{\\partial^2 f}{\\partial x^2}\\big|_\\bar{x} \\frac{1}{2} \\Delta x^2 $$"
      ]
    },
    {
      "metadata": {
        "id": "UcEAdMbDexlL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will successively improve our estimate of the minimum by choosing different points $\\bar{x}_n$ that are closer to the minimum. We want $\\bar{x}_n$ to be a stationary point, so that\n",
        "$$ \\frac{df(\\bar{x} + \\Delta x)}{d\\Delta x} = 0$$\n",
        "\n",
        "This happens when \n",
        "$$ \\frac{\\partial f}{\\partial x}\\big|_{\\bar{x}_n} + \\frac{\\partial^2 f}{\\partial x^2}\\big|_{\\bar{x}_n} \\Delta x = 0 $$\n",
        "\n",
        "So we update successively with this $\\Delta x$:\n",
        "$$ \\bar{x}_{n+1} = \\bar{x}_n + \\Delta x =\\bar{x}_n - \\frac{\\partial f}{\\partial x}\\big|_{\\bar{x}_n} / \\frac{\\partial^2 f}{\\partial x^2}\\big|_{\\bar{x}_n} $$\n"
      ]
    },
    {
      "metadata": {
        "id": "Q73EPZjMGnsg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f = lambda x : (x - 0.25)**2\n",
        "\n",
        "def D(f, x, h = 1e-6):\n",
        "    temp = x+h\n",
        "    h = temp-x\n",
        "    return (f(x+h) - f(x-h)) / (2*h)\n",
        "\n",
        "def DD(f, x, h = 1e-6):\n",
        "    temp = x+h\n",
        "    h = temp-x\n",
        "    return (f(x+h) + f(x-h) - 2*f(x)) / h**2\n",
        "\n",
        "\n",
        "def newton_optimize(f, xlow, xhigh, df = None, ddf = None, accuracy = 1e-6, nmax = 100):\n",
        "    xbar = xlow\n",
        "    fbar = f(xlow)\n",
        "    fhigh = f(xhigh)\n",
        "    if fhigh < fbar: \n",
        "        xbar, fbar = xhigh,fhigh\n",
        "\n",
        "    # if user doesn't supply 1st and 2nd derivatives\n",
        "    # make function objects to evaluate them\n",
        "    if df == None:\n",
        "        df = lambda x : D(f,x)\n",
        "    if ddf == None:\n",
        "        ddf = lambda x : DD(f,x)\n",
        "\n",
        "    for i in range(nmax):\n",
        "        x = xbar - df(xbar) / ddf(xbar)\n",
        "        print(i, xbar, x, df(xbar), ddf(xbar))\n",
        "        if math.fabs(x - xbar) < accuracy:\n",
        "            return x, f(x)\n",
        "        else:\n",
        "            xbar = x\n",
        "    raise ArithmeticError(\"Failed to converge\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MJIHiMemecb5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(newton_optimize(f, -1.0, +1.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HolX9gDdP0xx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Multiple dimensions"
      ]
    },
    {
      "metadata": {
        "id": "F6EBwq2eGi9U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####A test function: $f(\\vec{x})=(x-1)^2 + (y-2)^2$ "
      ]
    },
    {
      "metadata": {
        "id": "Tfw9t0LJGi9V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f = lambda x,y : (x - 1.0)**2 + (y - 2.0)**2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iBZgZdiZV-X5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"f(1.0, 1.0) = %f\" % f(1.0, 1.0))\n",
        "print(\"f(1.0, 2.0) = %f\" % f(1.0, 2.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xNnKHFixP0xy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###How to plot a 2D scalar function"
      ]
    },
    {
      "metadata": {
        "id": "eUiWwcx5P0x0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xrange = np.linspace(-4.0, +4.0, 100)\n",
        "yrange = np.linspace(-4.0, +4.0, 100)\n",
        "xpts, ypts = np.meshgrid(xrange, yrange)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g6rXRlYuWMEO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(xrange)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dThlyS2NWQRq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(xpts)\n",
        "print(ypts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VBormAfwXUua",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for x,y in zip(xpts,ypts):\n",
        "  print(f(x,y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aAWnWSSrXRBk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "[f(x,y) for x,y in zip(xpts,ypts)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rTJyDw12WXJD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fpts = np.array([f(x,y) for x,y in zip(xpts,ypts)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hXwLXpS4Up_0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection = '3d')\n",
        "ax.plot_surface(xpts, ypts, fpts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V9e5ntwQUNZA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.contour(xpts, ypts, fpts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IO6LZwcnUP6l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.contourf(xpts, ypts, fpts)\n",
        "plt.plot([1],[2], \"-or\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "phqQX1w3P0x4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###How to find the minimum?"
      ]
    },
    {
      "metadata": {
        "id": "v9dXldvfjgYn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you know, from calculus, we can expand any well behaved function $f(\\vec{x})$ in a series around a point $\\vec{x}$:\n",
        "\n",
        "$$f(\\vec{x} + \\Delta\\vec{x}) = f(\\vec{x}) + \\nabla f(\\vec{x})\\big|_\\vec{x} \\Delta\\vec{x} + \\frac{1}{2} H f(\\vec{x})\\big|_\\vec{x} \\Delta\\vec{x}^2 $$"
      ]
    },
    {
      "metadata": {
        "id": "W_OGIj3ojgYp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will successively improve our estimate of the minimum by choosing different points $\\bar{x}_n$ that are closer to the minimum. We want $\\bar{x}_n$ to be a stationary point, so that\n",
        "$$ \\frac{df(\\vec{x} + \\Delta\\vec{x})}{d\\Delta\\vec{x}} = 0$$\n",
        "\n",
        "This happens when \n",
        "$$ \\nabla f\\big|_{\\vec{x}} + H f\\big|_{\\vec{x}} \\Delta \\vec{x} = 0 $$\n",
        "\n",
        "So we update successively with this $\\Delta\\vec{x}$:\n",
        "$$ \\vec{x}_{n+1} = \\vec{x}_n + \\Delta \\vec{x} =\\vec{x}_n - \\big[ H f \\big|_{\\vec{x}_n} \\big]^{-1} \\nabla f\\big|_{\\vec{x}_n} $$\n"
      ]
    },
    {
      "metadata": {
        "id": "p-utFQFxP0x6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h-QWR2IGGnsi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Newton method for functions of more than one variable"
      ]
    },
    {
      "metadata": {
        "id": "82zft3JSGi9B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###The partial derivative"
      ]
    },
    {
      "metadata": {
        "id": "bmjfm7itXtdC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Consider the function $f(x_0,x_1,x_2) = 2 x_0 + 3 x_1 + 5 x_1 x_2$\n",
        "\n",
        "It is easier to consider the arguments $x_i$ to be part of a single vector argument $\\vec{x}$"
      ]
    },
    {
      "metadata": {
        "id": "QIgOqG7eGi9C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def partial(f, i, h = 1e-6):\n",
        "    ''' \n",
        "        Returns a function object to compute the partial derivative of f with respect to x[i].\n",
        "        \n",
        "        f(x) is assumed to be a scalar function of a vector or scalar argument x.\n",
        "    '''\n",
        "    def df(x, f = f, i = i, h = h):\n",
        "        x = np.array(x, dtype = np.float64) # make a copy and assure the use of 64-bit floats\n",
        "        x[i] += h\n",
        "        f_plus = f(x)\n",
        "        x[i] -= 2*h\n",
        "        f_minus = f(x)\n",
        "        return (f_plus - f_minus) / (2.0*h)\n",
        "    # note, partial() returns a function object, not the result of the function\n",
        "    return df\n",
        "\n",
        "fmulti = lambda x: 2*x[0] + 3*x[1] + 5*x[1]*x[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CLuDzQZPYjpD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df0 = partial(fmulti,2)\n",
        "v = [1,1,1]\n",
        "print(df0(v))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9mP0I99IYnOw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df1 = partial(fmulti,1)\n",
        "df2 = partial(fmulti,2)\n",
        "print(df1(v),df2(v))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lDMqrXq6YnDe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df12 = partial(df1,2)\n",
        "print(df12(v))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L7kcep_WGi9E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###A little bit about matrices"
      ]
    },
    {
      "metadata": {
        "id": "ZXgsv1G4Gi9E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now need to represent vector quantities such as $\\nabla f(\\vec{x})$ and the Hessian matrix $H$. This means using matrices and matrix algebra.  Here's a quick introduction:"
      ]
    },
    {
      "metadata": {
        "id": "b3_xs7G6Gi9F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy.matlib as ml\n",
        "\n",
        "# make a 2x2 (row x column) matrix\n",
        "M = ml.matrix([[1,2],[3,4]])\n",
        "print(M)\n",
        "print(type(M))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VTaybSDTYN6a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# make a 1x2 \"matrix\" (a.k.a. a row vector)\n",
        "V = ml.matrix([1,2])\n",
        "print(V)\n",
        "print(V.T) # the transpose of V = a 2x1 matrix = a column vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Gmbq69fYNZH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(M.dot(V.T)) # the dot product\n",
        "print(M.I) # the matrix inverse\n",
        "print(M.I * M) # checking the inverse. * is a other way to take the dot product"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YBmXaBsU_31g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###The gradient"
      ]
    },
    {
      "metadata": {
        "id": "k_8zTMaz_31l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gradient(f, x, h = 1e-6):\n",
        "    ''' return the gradient of f(x) as a column vector with length = len(x) '''\n",
        "    v = ml.matrix([partial(f, i, h = h)(x) for i in range(len(x))], dtype = np.float64)\n",
        "    return v.T\n",
        "\n",
        "print(gradient(fmulti,v))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C7S0FOvn_31q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###The Hessian matrix"
      ]
    },
    {
      "metadata": {
        "id": "mIuC19CyY80w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/ceb2ef7133d4ffb011021db5f90126d42058378d\">"
      ]
    },
    {
      "metadata": {
        "id": "S97ceSpw_31q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def hessian(f, x, h = 1e-6):\n",
        "    ''' returns the two dimensional matrix of second partial derivatives of f(x). a.k.a. the Hessian matrix '''\n",
        "    v = [ [ partial(partial(f,column),row)(x) for column in range(len(x)) ] for row in range(len(x))]\n",
        "    mx = ml.matrix(v, dtype = np.float64)\n",
        "    return ml.matrix(v,dtype = np.float64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MI_YQ_OV_32L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####A test function: $f(\\vec{x})=(x-1)^2 + (y-2)^2$ "
      ]
    },
    {
      "metadata": {
        "id": "1gtE2z_PZxZ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using obscure `numpy.mgrid` tricks: https://docs.scipy.org/doc/numpy/reference/generated/numpy.mgrid.html"
      ]
    },
    {
      "metadata": {
        "id": "R3jUp4RB_32M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f = lambda x : (x[0]-1)**2 + (x[1]-2)**2\n",
        "x, y = np.mgrid[-2.5:3:101j, 0:4:101j]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ecreO3eE_32T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####The `multi_newton` function"
      ]
    },
    {
      "metadata": {
        "id": "tJfAHwvM_32W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def multi_newton(f, xguess, h = 1e-6, accuracy = 1e-6, nmax=100, want_points = False, debug = False):\n",
        "    xbar = ml.matrix(xguess, dtype = np.float64).T # to get a column vector\n",
        "    xpoints = [xbar.A1[0]]\n",
        "    ypoints = [xbar.A1[1]]\n",
        "    for i in range(nmax):\n",
        "        H = hessian(f, xbar.A1)\n",
        "        grad = gradient(f, xbar.A1) # to get a column vector\n",
        "        if debug:\n",
        "            print(\"iteration \", i)\n",
        "            print(\"xbar\", xbar)\n",
        "            print(\"grad=\", grad)\n",
        "            print(\"H=\", H)\n",
        "            print(\"H.I=\", H.I)\n",
        "            print(\"H.I.dot(grad)\", H.I.dot(grad))\n",
        "        x = xbar - H.I.dot(grad)\n",
        "        if debug: print(\"x=\", x)\n",
        "        xpoints.append(x.A1[0])\n",
        "        ypoints.append(x.A1[1])\n",
        "        if np.sum((x.A1 - xbar.A1)**2) < accuracy**2:\n",
        "            if not want_points:\n",
        "                return x.A1\n",
        "            else:\n",
        "                return x.A1, xpoints, ypoints\n",
        "        else:\n",
        "            xbar = x\n",
        "    raise ArithmeticError(\"Failed to converge\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TIrFa5cV_32b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####A somewhat more difficult test function"
      ]
    },
    {
      "metadata": {
        "id": "yJggLZpe_32c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fharder = lambda x : 1 - x[0]*np.exp(-x[0]) + (x[1] - 2)**2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YsMNEjiB_32f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Testing `multi_newton` on the new test function"
      ]
    },
    {
      "metadata": {
        "id": "rWLq2cdq_32f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "minx, xpoints, ypoints = multi_newton(fharder, [1.8,2.5], want_points = True)\n",
        "cs = plt.contourf(X, Y, fharder([X,Y]), 100)\n",
        "plt.colorbar(cs)\n",
        "print(xpoints, ypoints)\n",
        "plt.plot(xpoints, ypoints, \"-ow\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}